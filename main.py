# í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹…
print("ğŸ” í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹…:")
print(f"NAVER_CLIENT_ID ì¡´ì¬ ì—¬ë¶€: {os.getenv('NAVER_CLIENT_ID') is not None}")
print(f"NAVER_CLIENT_SECRET ì¡´ì¬ ì—¬ë¶€: {os.getenv('NAVER_CLIENT_SECRET') is not None}")
print(f"OPENAI_API_KEY ì¡´ì¬ ì—¬ë¶€: {os.getenv('OPENAI_API_KEY') is not None}")
print(f"NOTION_TOKEN ì¡´ì¬ ì—¬ë¶€: {os.getenv('NOTION_TOKEN') is not None}")
print(f"NOTION_DATABASE_ID ì¡´ì¬ ì—¬ë¶€: {os.getenv('NOTION_DATABASE_ID') is not None}")

# ëª¨ë“  í™˜ê²½ë³€ìˆ˜ ì¶œë ¥ (ê°’ì€ ìˆ¨ê¹€)
all_env = dict(os.environ)
naver_vars = {k: v for k, v in all_env.items() if 'NAVER' in k}
print(f"ë„¤ì´ë²„ ê´€ë ¨ í™˜ê²½ë³€ìˆ˜: {list(naver_vars.keys())}")

import subprocess
import sys

# íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜
def install_if_missing(package):
    try:
        __import__(package.replace("-", "_"))
    except ImportError:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ ì„¤ì¹˜
packages = ["requests", "python-dotenv", "beautifulsoup4", "openai", "notion-client"]
for pkg in packages:
    install_if_missing(pkg)

import os
import datetime
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import openai
from notion_client import Client

# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
#load_dotenv()

# API í‚¤ ì„¤ì •
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
NOTION_DB_ID = os.getenv("NOTION_DATABASE_ID")

# í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸
required_vars = {
    "NAVER_CLIENT_ID": NAVER_CLIENT_ID,
    "NAVER_CLIENT_SECRET": NAVER_CLIENT_SECRET,
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "NOTION_TOKEN": NOTION_TOKEN,
    "NOTION_DATABASE_ID": NOTION_DB_ID
}

missing_vars = [key for key, value in required_vars.items() if not value]
if missing_vars:
    print(f"âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}")
    sys.exit(1)

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai.api_key = OPENAI_API_KEY
notion = Client(auth=NOTION_TOKEN)

print("âœ… ëª¨ë“  í™˜ê²½ë³€ìˆ˜ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# 1. ë„¤ì´ë²„ ë°ì´í„°ë© APIë¡œ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
def get_trending_keywords():
    try:
        url = "https://openapi.naver.com/v1/datalab/search"
        headers = {
            'X-Naver-Client-Id': NAVER_CLIENT_ID,
            'X-Naver-Client-Secret': NAVER_CLIENT_SECRET,
            'Content-Type': 'application/json'
        }
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ 30ì¼ê°„ ë°ì´í„° ìš”ì²­
        end_date = datetime.date.today().strftime('%Y-%m-%d')
        start_date = (datetime.date.today() - datetime.timedelta(days=30)).strftime('%Y-%m-%d')
        
        # ì¸ê¸° ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ë“¤
        categories = ["ì¸ê³µì§€ëŠ¥", "íˆ¬ì", "ë¶€ë™ì‚°", "ì·¨ì—…", "ì—¬í–‰"]
        
        body = {
            "startDate": start_date,
            "endDate": end_date,
            "timeUnit": "date",
            "keywordGroups": [
                {
                    "groupName": category,
                    "keywords": [category]
                } for category in categories
            ]
        }
        
        response = requests.post(url, headers=headers, json=body)
        
        if response.status_code == 200:
            data = response.json()
            # ê²€ìƒ‰ëŸ‰ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            keywords = [group['title'] for group in data['results']]
            print(f"âœ… ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {keywords}")
            return keywords[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
        else:
            print(f"âŒ ë„¤ì´ë²„ ë°ì´í„°ë© API ì˜¤ë¥˜: {response.status_code}")
            raise Exception("API í˜¸ì¶œ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ë°ì´í„°ë© ì˜¤ë¥˜: {e}")
        # ë°±ì—… í‚¤ì›Œë“œ ì‚¬ìš©
        backup_keywords = ["ì¸ê³µì§€ëŠ¥", "íˆ¬ì", "ë¶€ë™ì‚°", "ì·¨ì—…", "ì—¬í–‰"]
        print(f"ğŸ”„ ë°±ì—… í‚¤ì›Œë“œ ì‚¬ìš©: {backup_keywords}")
        return backup_keywords

# 2. êµ¬ê¸€ì—ì„œ í‚¤ì›Œë“œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
def collect_google_data(keyword):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        # êµ¬ê¸€ ê²€ìƒ‰
        search_url = f"https://www.google.com/search?q={keyword}"
        response = requests.get(search_url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê²€ìƒ‰ ê²°ê³¼ íƒ€ì´í‹€ ìˆ˜ì§‘
        titles = []
        for result in soup.find_all('h3', limit=5):
            if result.text:
                titles.append(result.text)
        
        # êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰
        news_url = f"https://www.google.com/search?q={keyword}&tbm=nws"
        news_response = requests.get(news_url, headers=headers)
        news_soup = BeautifulSoup(news_response.text, 'html.parser')
        
        # ë‰´ìŠ¤ ì œëª© ìˆ˜ì§‘
        news_titles = []
        for news in news_soup.find_all('h3', limit=3):
            if news.text:
                news_titles.append(news.text)
        
        collected_data = {
            'search_results': titles,
            'news_results': news_titles
        }
        
        print(f"âœ… '{keyword}' êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return collected_data
        
    except Exception as e:
        print(f"âŒ '{keyword}' êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {'search_results': [], 'news_results': []}

# 3. GPTë¡œ ë°ì´í„° ë¶„ì„
def analyze_with_gpt(keyword, google_data):
    try:
        search_text = " / ".join(google_data['search_results'])
        news_text = " / ".join(google_data['news_results'])
        
        prompt = f"""
ë‹¤ìŒì€ '{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ êµ¬ê¸€ ê²€ìƒ‰ ë° ë‰´ìŠ¤ ë°ì´í„°ì…ë‹ˆë‹¤:

ê²€ìƒ‰ ê²°ê³¼: {search_text}
ë‰´ìŠ¤ ê²°ê³¼: {news_text}

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
ë‹ˆì¦ˆ: [ì´ íŠ¸ë Œë“œê°€ ë°˜ì˜í•˜ëŠ” ì†Œë¹„ì ë‹ˆì¦ˆë‚˜ ì‹¬ë¦¬ì  ìš•êµ¬]
ìš”ì•½: [ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ 1-2ì¤„ ìš”ì•½]
ì „ë§: [í–¥í›„ íŠ¸ë Œë“œ ì „ë§]
"""
        
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        
        result = response.choices[0].message.content.strip()
        print(f"âœ… '{keyword}' GPT ë¶„ì„ ì™„ë£Œ")
        return result
        
    except Exception as e:
        print(f"âŒ '{keyword}' GPT ë¶„ì„ ì‹¤íŒ¨: {e}")
        return f"ë‹ˆì¦ˆ: ë¶„ì„ ì‹¤íŒ¨\nìš”ì•½: {keyword}ì— ëŒ€í•œ ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\nì „ë§: ì¶”í›„ ì¬ë¶„ì„ í•„ìš”"

# 4. ë…¸ì…˜ì— ê²°ê³¼ ì—…ë¡œë“œ
def upload_to_notion(keyword, analysis_result):
    try:
        today = datetime.date.today().isoformat()
        
        # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
        lines = analysis_result.split('\n')
        parsed_data = {"í‚¤ì›Œë“œ": keyword, "ë‚ ì§œ": today}
        
        for line in lines:
            if line.startswith("ë‹ˆì¦ˆ:"):
                parsed_data["ë‹ˆì¦ˆ"] = line.replace("ë‹ˆì¦ˆ:", "").strip()
            elif line.startswith("ìš”ì•½:"):
                parsed_data["ìš”ì•½"] = line.replace("ìš”ì•½:", "").strip()
            elif line.startswith("ì „ë§:"):
                parsed_data["ì „ë§"] = line.replace("ì „ë§:", "").strip()
        
        # ë…¸ì…˜ í˜ì´ì§€ ìƒì„±
        notion.pages.create(
            parent={"database_id": NOTION_DB_ID},
            properties={
                "í‚¤ì›Œë“œ": {"title": [{"text": {"content": parsed_data["í‚¤ì›Œë“œ"]}}]},
                "ë‚ ì§œ": {"date": {"start": parsed_data["ë‚ ì§œ"]}},
                "ë‹ˆì¦ˆ": {"rich_text": [{"text": {"content": parsed_data.get("ë‹ˆì¦ˆ", "")}}]},
                "ìš”ì•½": {"rich_text": [{"text": {"content": parsed_data.get("ìš”ì•½", "")}}]},
                "ì „ë§": {"rich_text": [{"text": {"content": parsed_data.get("ì „ë§", "")}}]},
            },
        )
        
        print(f"âœ… '{keyword}' ë…¸ì…˜ ì—…ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ '{keyword}' ë…¸ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

# 5. ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
def main():
    print("ğŸš€ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
    
    # 1. íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘
    keywords = get_trending_keywords()
    
    # 2. ê° í‚¤ì›Œë“œë³„ ë¶„ì„
    for keyword in keywords:
        print(f"\nğŸ“Š '{keyword}' ë¶„ì„ ì¤‘...")
        
        # êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘
        google_data = collect_google_data(keyword)
        
        # GPT ë¶„ì„
        analysis = analyze_with_gpt(keyword, google_data)
        
        # ë…¸ì…˜ ì—…ë¡œë“œ
        upload_to_notion(keyword, analysis)
        
        print(f"âœ… '{keyword}' ë¶„ì„ ì™„ë£Œ")
    
    print("\nğŸ‰ ëª¨ë“  íŠ¸ë Œë“œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
