import os
import sys
import datetime
import subprocess

# íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜ (Renderì—ì„œ ì•ˆì „í•˜ê²Œ)
def install_package(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package], 
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
    except:
        print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨")

# í•„ìš” íŒ¨í‚¤ì§€ í™•ì¸ ë° ì„¤ì¹˜
required_packages = {
    'requests': 'requests',
    'bs4': 'beautifulsoup4', 
    'openai': 'openai',
    'notion_client': 'notion-client'
}

for module, package in required_packages.items():
    try:
        __import__(module)
    except ImportError:
        print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
        install_package(package)

# íŒ¨í‚¤ì§€ import
import requests
from bs4 import BeautifulSoup
import openai
from notion_client import Client

print("ğŸš€ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘...")

# í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (Renderì—ì„œ)
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
NOTION_DB_ID = os.environ.get("NOTION_DATABASE_ID")

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
env_vars = {
    "NAVER_CLIENT_ID": NAVER_CLIENT_ID,
    "NAVER_CLIENT_SECRET": NAVER_CLIENT_SECRET,
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "NOTION_TOKEN": NOTION_TOKEN,
    "NOTION_DATABASE_ID": NOTION_DB_ID
}

missing_vars = [name for name, value in env_vars.items() if not value]
if missing_vars:
    print(f"âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing_vars)}")
    print("ğŸ”§ Render Dashboard â†’ Settings â†’ Environmentì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
    sys.exit(1)

print("âœ… ëª¨ë“  í™˜ê²½ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ")

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    notion_client = Client(auth=NOTION_TOKEN)
    print("âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    print(f"âŒ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    sys.exit(1)

# 1. íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ (ë°±ì—… ë°©ì‹)
def get_trending_keywords():
    try:
        # ë„¤ì´ë²„ ë°ì´í„°ë© API ì‹œë„
        url = "https://openapi.naver.com/v1/datalab/search"
        headers = {
            'X-Naver-Client-Id': NAVER_CLIENT_ID,
            'X-Naver-Client-Secret': NAVER_CLIENT_SECRET,
            'Content-Type': 'application/json'
        }
        
        end_date = datetime.date.today().strftime('%Y-%m-%d')
        start_date = (datetime.date.today() - datetime.timedelta(days=7)).strftime('%Y-%m-%d')
        
        body = {
            "startDate": start_date,
            "endDate": end_date,
            "timeUnit": "date",
            "keywordGroups": [
                {"groupName": "AI", "keywords": ["ì¸ê³µì§€ëŠ¥"]},
                {"groupName": "íˆ¬ì", "keywords": ["íˆ¬ì"]},
                {"groupName": "ë¶€ë™ì‚°", "keywords": ["ë¶€ë™ì‚°"]}
            ]
        }
        
        response = requests.post(url, headers=headers, json=body, timeout=10)
        
        if response.status_code == 200:
            keywords = ["ì¸ê³µì§€ëŠ¥", "íˆ¬ì", "ë¶€ë™ì‚°"]
            print("âœ… ë„¤ì´ë²„ ë°ì´í„°ë© í‚¤ì›Œë“œ ìˆ˜ì§‘ ì„±ê³µ")
            return keywords
        else:
            raise Exception(f"API ì˜¤ë¥˜: {response.status_code}")
            
    except Exception as e:
        print(f"âš ï¸ ë„¤ì´ë²„ API ì‹¤íŒ¨: {e}")
        # í˜„ì¬ ì´ìŠˆ í‚¤ì›Œë“œ ì‚¬ìš©
        trending_keywords = ["ChatGPT", "ë¹„íŠ¸ì½”ì¸", "ë¶€ë™ì‚° íˆ¬ì", "ì·¨ì—… ì¤€ë¹„", "ì—¬í–‰"]
        print(f"ğŸ“ˆ í˜„ì¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì‚¬ìš©: {trending_keywords}")
        return trending_keywords

# 2. êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘
def collect_google_data(keyword):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        # êµ¬ê¸€ ê²€ìƒ‰
        search_url = f"https://www.google.com/search?q={keyword}&hl=ko"
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê²€ìƒ‰ ê²°ê³¼ ì œëª© ìˆ˜ì§‘
        titles = []
        for h3 in soup.find_all('h3', limit=3):
            if h3.text.strip():
                titles.append(h3.text.strip())
        
        if not titles:
            titles = [f"{keyword} ê´€ë ¨ ìµœì‹  ë™í–¥", f"{keyword} ì‹œì¥ ë¶„ì„", f"{keyword} ì „ë§"]
        
        return {
            'search_results': titles,
            'keyword': keyword
        }
        
    except Exception as e:
        print(f"âš ï¸ êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({keyword}): {e}")
        return {
            'search_results': [f"{keyword} íŠ¸ë Œë“œ ë¶„ì„", f"{keyword} ì‹œì¥ ë™í–¥"],
            'keyword': keyword
        }

# 3. GPT ë¶„ì„
def analyze_with_gpt(keyword, google_data):
    try:
        search_text = " / ".join(google_data['search_results'])
        
        prompt = f"""
í‚¤ì›Œë“œ: {keyword}
ê´€ë ¨ ì •ë³´: {search_text}

ìœ„ í‚¤ì›Œë“œì™€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

ë‹ˆì¦ˆ: [ì´ í‚¤ì›Œë“œê°€ ë°˜ì˜í•˜ëŠ” ì†Œë¹„ìì˜ ì£¼ìš” ë‹ˆì¦ˆë‚˜ ê´€ì‹¬ì‚¬]
ìš”ì•½: [ë§ˆì¼€íŒ… ê´€ì ì—ì„œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 1-2ì¤„]
ì „ë§: [í–¥í›„ 3-6ê°œì›” íŠ¸ë Œë“œ ì „ë§]
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        result = response.choices[0].message.content.strip()
        print(f"âœ… '{keyword}' GPT ë¶„ì„ ì™„ë£Œ")
        return result
        
    except Exception as e:
        print(f"âš ï¸ '{keyword}' GPT ë¶„ì„ ì‹¤íŒ¨: {e}")
        return f"""ë‹ˆì¦ˆ: {keyword}ì— ëŒ€í•œ ì •ë³´ ìˆ˜ìš” ì¦ê°€
ìš”ì•½: {keyword} ê´€ë ¨ ê´€ì‹¬ë„ê°€ ë†’ì•„ì§€ê³  ìˆì–´ ë§ˆì¼€íŒ… ê¸°íšŒ ì¡´ì¬
ì „ë§: ì§€ì†ì ì¸ ê´€ì‹¬ ìœ ì§€ ì˜ˆìƒ"""

# 4. ë…¸ì…˜ ì—…ë¡œë“œ
def upload_to_notion(keyword, analysis_result):
    try:
        today = datetime.date.today().isoformat()
        
        # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
        lines = analysis_result.split('\n')
        parsed_data = {
            "í‚¤ì›Œë“œ": keyword,
            "ë‚ ì§œ": today,
            "ë‹ˆì¦ˆ": "",
            "ìš”ì•½": "",
            "ì „ë§": ""
        }
        
        for line in lines:
            line = line.strip()
            if line.startswith("ë‹ˆì¦ˆ:"):
                parsed_data["ë‹ˆì¦ˆ"] = line.replace("ë‹ˆì¦ˆ:", "").strip()
            elif line.startswith("ìš”ì•½:"):
                parsed_data["ìš”ì•½"] = line.replace("ìš”ì•½:", "").strip()
            elif line.startswith("ì „ë§:"):
                parsed_data["ì „ë§"] = line.replace("ì „ë§:", "").strip()
        
        # ë…¸ì…˜ í˜ì´ì§€ ìƒì„±
        notion_client.pages.create(
            parent={"database_id": NOTION_DB_ID},
            properties={
                "í‚¤ì›Œë“œ": {"title": [{"text": {"content": parsed_data["í‚¤ì›Œë“œ"]}}]},
                "ë‚ ì§œ": {"date": {"start": parsed_data["ë‚ ì§œ"]}},
                "ë‹ˆì¦ˆ": {"rich_text": [{"text": {"content": parsed_data["ë‹ˆì¦ˆ"]}}]},
                "ìš”ì•½": {"rich_text": [{"text": {"content": parsed_data["ìš”ì•½"]}}]},
                "ì „ë§": {"rich_text": [{"text": {"content": parsed_data["ì „ë§"]}}]},
            },
        )
        
        print(f"âœ… '{keyword}' ë…¸ì…˜ ì—…ë¡œë“œ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ '{keyword}' ë…¸ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    print("="*50)
    print("ğŸ¤– ìë™ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    try:
        # 1. í‚¤ì›Œë“œ ìˆ˜ì§‘
        keywords = get_trending_keywords()
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ: {keywords}")
        
        success_count = 0
        
        # 2. ê° í‚¤ì›Œë“œë³„ ë¶„ì„
        for i, keyword in enumerate(keywords, 1):
            print(f"\n[{i}/{len(keywords)}] ğŸ” '{keyword}' ë¶„ì„ ì¤‘...")
            
            # êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘
            google_data = collect_google_data(keyword)
            
            # GPT ë¶„ì„
            analysis = analyze_with_gpt(keyword, google_data)
            
            # ë…¸ì…˜ ì—…ë¡œë“œ
            if upload_to_notion(keyword, analysis):
                success_count += 1
            
            print(f"âœ… '{keyword}' ì²˜ë¦¬ ì™„ë£Œ")
        
        print("\n" + "="*50)
        print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(keywords)}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
