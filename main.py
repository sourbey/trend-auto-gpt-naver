import os
import sys
import json
import time
import datetime
import requests
from typing import List, Dict, Any
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import openai
from notion_client import Client

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET") 
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
NOTION_DB_ID = os.getenv("NOTION_DATABASE_ID")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai.api_key = OPENAI_API_KEY
notion = Client(auth=NOTION_TOKEN)

class TrendAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def get_naver_trending_keywords(self) -> List[str]:
        """ë„¤ì´ë²„ ë°ì´í„°ë© APIë¡œ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘"""
        try:
            # ë„¤ì´ë²„ ë°ì´í„°ë© ì‹¤ì‹œê°„ ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´ API
            url = "https://openapi.naver.com/v1/datalab/search"
            
            headers = {
                'X-Naver-Client-Id': NAVER_CLIENT_ID,
                'X-Naver-Client-Secret': NAVER_CLIENT_SECRET,
                'Content-Type': 'application/json'
            }
            
            # ìµœê·¼ 7ì¼ê°„ ì¸ê¸° ê²€ìƒ‰ì–´ ìš”ì²­
            end_date = datetime.date.today()
            start_date = end_date - datetime.timedelta(days=7)
            
            body = {
                "startDate": start_date.strftime("%Y-%m-%d"),
                "endDate": end_date.strftime("%Y-%m-%d"),
                "timeUnit": "date",
                "keywordGroups": [
                    {"groupName": "íŠ¸ë Œë“œ", "keywords": ["íŠ¸ë Œë“œ"]}
                ]
            }
            
            response = self.session.post(url, headers=headers, data=json.dumps(body))
            
            if response.status_code == 200:
                # API ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
                # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ íŒŒì‹±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
                print("ë„¤ì´ë²„ ë°ì´í„°ë© API í˜¸ì¶œ ì„±ê³µ")
                
                # ëŒ€ì•ˆ: ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í˜ì´ì§€ í¬ë¡¤ë§
                return self.crawl_naver_realtime_keywords()
            else:
                print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return self.get_fallback_keywords()
                
        except Exception as e:
            print(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {e}")
            return self.get_fallback_keywords()
    
    def crawl_naver_realtime_keywords(self) -> List[str]:
        """ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í¬ë¡¤ë§ (ëŒ€ì•ˆ)"""
        try:
            # ë„¤ì´ë²„ ë°ì´í„°ë© í‚¤ì›Œë“œ í˜ì´ì§€
            url = "https://datalab.naver.com/keyword/realtimeList.naver?where=main"
            
            response = self.session.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì¶”ì¶œ (CSS ì„ íƒìëŠ” í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            keywords = []
            keyword_elements = soup.select('.keyword_rank li .title')
            
            if keyword_elements:
                keywords = [elem.get_text(strip=True) for elem in keyword_elements[:10]]
            
            if keywords:
                print(f"ë„¤ì´ë²„ ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì„±ê³µ: {keywords[:5]}")
                return keywords[:5]
            else:
                return self.get_fallback_keywords()
                
        except Exception as e:
            print(f"ë„¤ì´ë²„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return self.get_fallback_keywords()
    
    def get_fallback_keywords(self) -> List[str]:
        """ë°±ì—… í‚¤ì›Œë“œ (API ì‹¤íŒ¨ì‹œ ì‚¬ìš©)"""
        fallback = [
            "ì¸ê³µì§€ëŠ¥ ChatGPT", 
            "ë¶€ë™ì‚° íˆ¬ì", 
            "ì·¨ì—… ì¤€ë¹„", 
            "ê±´ê°• ê´€ë¦¬",
            "ì—¬í–‰ ê³„íš"
        ]
        print(f"ë°±ì—… í‚¤ì›Œë“œ ì‚¬ìš©: {fallback}")
        return fallback
    
    def search_google_content(self, keyword: str) -> Dict[str, Any]:
        """êµ¬ê¸€ì—ì„œ í‚¤ì›Œë“œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
        try:
            # êµ¬ê¸€ ê²€ìƒ‰ URL êµ¬ì„±
            search_url = f"https://www.google.com/search?q={keyword}&num=10&hl=ko"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            }
            
            response = self.session.get(search_url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
            results = []
            search_results = soup.select('div.g')
            
            for result in search_results[:5]:
                title_elem = result.select_one('h3')
                snippet_elem = result.select_one('.VwiC3b, .s3v9rd')
                link_elem = result.select_one('a')
                
                if title_elem and snippet_elem:
                    results.append({
                        'title': title_elem.get_text(strip=True),
                        'snippet': snippet_elem.get_text(strip=True),
                        'link': link_elem.get('href') if link_elem else ''
                    })
            
            # ì—°ê´€ ê²€ìƒ‰ì–´ ì¶”ì¶œ
            related_searches = []
            related_elements = soup.select('.s75CSd .sATSHe')
            for elem in related_elements[:5]:
                related_searches.append(elem.get_text(strip=True))
            
            return {
                'keyword': keyword,
                'results': results,
                'related_searches': related_searches,
                'total_results': len(results)
            }
            
        except Exception as e:
            print(f"êµ¬ê¸€ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
            return {
                'keyword': keyword,
                'results': [],
                'related_searches': [],
                'total_results': 0
            }
    
    def analyze_with_gpt(self, keyword: str, search_data: Dict[str, Any]) -> Dict[str, str]:
        """GPTë¡œ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ì •ë¦¬
            content_summary = ""
            for result in search_data['results']:
                content_summary += f"ì œëª©: {result['title']}\në‚´ìš©: {result['snippet']}\n\n"
            
            related_info = ", ".join(search_data['related_searches'])
            
            # GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒì€ '{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ìµœì‹  ê²€ìƒ‰ ì •ë³´ì…ë‹ˆë‹¤:

=== ê²€ìƒ‰ ê²°ê³¼ ===
{content_summary}

=== ì—°ê´€ ê²€ìƒ‰ì–´ ===
{related_info}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **íŠ¸ë Œë“œ ë°°ê²½**: ì´ í‚¤ì›Œë“œê°€ ì™œ ì¸ê¸°ë¥¼ ëŒê³  ìˆëŠ”ì§€
2. **ì†Œë¹„ì ë‹ˆì¦ˆ**: ì‚¬ëŒë“¤ì´ ì´ í‚¤ì›Œë“œë¥¼ í†µí•´ ì–»ìœ¼ë ¤ëŠ” ì •ë³´ë‚˜ í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œ
3. **ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸**: ë¹„ì¦ˆë‹ˆìŠ¤ë‚˜ ë§ˆì¼€íŒ… ê´€ì ì—ì„œì˜ í™œìš© ë°©ì•ˆ
4. **í–¥í›„ ì „ë§**: ì´ íŠ¸ë Œë“œì˜ ì§€ì†ì„±ê³¼ ë°œì „ ë°©í–¥

ê° í•­ëª©ì„ 2-3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""

            client = openai.OpenAI(api_key=OPENAI_API_KEY)
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            analysis = response.choices[0].message.content.strip()
            
            # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
            analysis_dict = {
                'keyword': keyword,
                'full_analysis': analysis,
                'trend_background': '',
                'consumer_needs': '',
                'marketing_insights': '',
                'future_outlook': ''
            }
            
            # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            lines = analysis.split('\n')
            current_section = ''
            
            for line in lines:
                line = line.strip()
                if 'íŠ¸ë Œë“œ ë°°ê²½' in line:
                    current_section = 'trend_background'
                elif 'ì†Œë¹„ì ë‹ˆì¦ˆ' in line:
                    current_section = 'consumer_needs'
                elif 'ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸' in line:
                    current_section = 'marketing_insights'
                elif 'í–¥í›„ ì „ë§' in line:
                    current_section = 'future_outlook'
                elif line and current_section:
                    analysis_dict[current_section] += line + ' '
            
            print(f"GPT ë¶„ì„ ì™„ë£Œ: {keyword}")
            return analysis_dict
            
        except Exception as e:
            print(f"GPT ë¶„ì„ ì˜¤ë¥˜ ({keyword}): {e}")
            return {
                'keyword': keyword,
                'full_analysis': f'{keyword}ì— ëŒ€í•œ ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'trend_background': 'ë¶„ì„ ë¶ˆê°€',
                'consumer_needs': 'ë¶„ì„ ë¶ˆê°€',
                'marketing_insights': 'ë¶„ì„ ë¶ˆê°€',
                'future_outlook': 'ë¶„ì„ ë¶ˆê°€'
            }
    
    def upload_to_notion(self, analysis: Dict[str, str], search_data: Dict[str, Any]) -> bool:
        """ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ì— ë¶„ì„ ê²°ê³¼ ì—…ë¡œë“œ"""
        try:
            today = datetime.date.today().isoformat()
            
            # ë…¸ì…˜ í˜ì´ì§€ ì†ì„± êµ¬ì„±
            properties = {
                "í‚¤ì›Œë“œ": {"title": [{"text": {"content": analysis['keyword']}}]},
                "ë¶„ì„ì¼ì": {"date": {"start": today}},
                "íŠ¸ë Œë“œ ë°°ê²½": {
                    "rich_text": [{"text": {"content": analysis.get('trend_background', '')}}]
                },
                "ì†Œë¹„ì ë‹ˆì¦ˆ": {
                    "rich_text": [{"text": {"content": analysis.get('consumer_needs', '')}}]
                },
                "ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸": {
                    "rich_text": [{"text": {"content": analysis.get('marketing_insights', '')}}]
                },
                "í–¥í›„ ì „ë§": {
                    "rich_text": [{"text": {"content": analysis.get('future_outlook', '')}}]
                },
                "ê²€ìƒ‰ ê²°ê³¼ ìˆ˜": {"number": search_data['total_results']},
                "ìƒíƒœ": {"select": {"name": "ì™„ë£Œ"}}
            }
            
            # í˜ì´ì§€ ì½˜í…ì¸  (ìƒì„¸ ë¶„ì„)
            children = [
                {
                    "object": "block",
                    "type": "heading_2",
                    "heading_2": {
                        "rich_text": [{"type": "text", "text": {"content": "ìƒì„¸ ë¶„ì„"}}]
                    }
                },
                {
                    "object": "block", 
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": analysis['full_analysis']}}]
                    }
                }
            ]
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
            if search_data['results']:
                children.append({
                    "object": "block",
                    "type": "heading_3", 
                    "heading_3": {
                        "rich_text": [{"type": "text", "text": {"content": "ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼"}}]
                    }
                })
                
                for result in search_data['results'][:3]:
                    children.append({
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [
                                {"type": "text", "text": {"content": f"â€¢ {result['title']}: {result['snippet'][:100]}..."}}
                            ]
                        }
                    })
            
            # ë…¸ì…˜ í˜ì´ì§€ ìƒì„±
            response = notion.pages.create(
                parent={"database_id": NOTION_DB_ID},
                properties=properties,
                children=children
            )
            
            print(f"ë…¸ì…˜ ì—…ë¡œë“œ ì™„ë£Œ: {analysis['keyword']}")
            return True
            
        except Exception as e:
            print(f"ë…¸ì…˜ ì—…ë¡œë“œ ì˜¤ë¥˜ ({analysis['keyword']}): {e}")
            return False
    
    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 50)
        
        # 1. íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘
        print("ğŸ“Š ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì¤‘...")
        keywords = self.get_naver_trending_keywords()
        
        if not keywords:
            print("âŒ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨")
            return
        
        print(f"âœ… ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ: {keywords}")
        
        # 2. ê° í‚¤ì›Œë“œ ë¶„ì„
        success_count = 0
        for i, keyword in enumerate(keywords, 1):
            print(f"\n[{i}/{len(keywords)}] '{keyword}' ë¶„ì„ ì¤‘...")
            
            # êµ¬ê¸€ ê²€ìƒ‰
            print("ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì¤‘...")
            search_data = self.search_google_content(keyword)
            
            if search_data['total_results'] == 0:
                print(f"âš ï¸ '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue
            
            # GPT ë¶„ì„
            print("ğŸ¤– GPT ë¶„ì„ ì¤‘...")
            analysis = self.analyze_with_gpt(keyword, search_data)
            
            # ë…¸ì…˜ ì—…ë¡œë“œ
            print("ğŸ“ ë…¸ì…˜ ì—…ë¡œë“œ ì¤‘...")
            if self.upload_to_notion(analysis, search_data):
                success_count += 1
                print(f"âœ… '{keyword}' ë¶„ì„ ì™„ë£Œ")
            else:
                print(f"âŒ '{keyword}' ì—…ë¡œë“œ ì‹¤íŒ¨")
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            time.sleep(2)
        
        print("\n" + "=" * 50)
        print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {success_count}/{len(keywords)}ê°œ ì„±ê³µ")
        print("ğŸ“Š ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ë³€ìˆ˜ ê²€ì¦
    required_vars = [
        'NAVER_CLIENT_ID', 'NAVER_CLIENT_SECRET', 
        'OPENAI_API_KEY', 'NOTION_TOKEN', 'NOTION_DATABASE_ID'
    ]
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        print(f"âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {missing_vars}")
        print("ğŸ’¡ .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!")
        return
    
    # íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰
    analyzer = TrendAnalyzer()
    analyzer.run_analysis()

if __name__ == "__main__":
    main()
