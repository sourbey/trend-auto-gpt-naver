import os
import sys
import datetime
import subprocess

# íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜ (Renderì—ì„œ ì•ˆì „í•˜ê²Œ)
def install_package(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package], 
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
    except:
        print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨")

# í•„ìš” íŒ¨í‚¤ì§€ í™•ì¸ ë° ì„¤ì¹˜
required_packages = {
    'requests': 'requests',
    'bs4': 'beautifulsoup4', 
    'openai': 'openai',
    'notion_client': 'notion-client'
}

for module, package in required_packages.items():
    try:
        __import__(module)
    except ImportError:
        print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
        install_package(package)

# íŒ¨í‚¤ì§€ import
import requests
from bs4 import BeautifulSoup
import openai
from notion_client import Client

print("ğŸš€ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘...")

# í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (Renderì—ì„œ)
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
NOTION_DB_ID = os.environ.get("NOTION_DATABASE_ID")

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
env_vars = {
    "NAVER_CLIENT_ID": NAVER_CLIENT_ID,
    "NAVER_CLIENT_SECRET": NAVER_CLIENT_SECRET,
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "NOTION_TOKEN": NOTION_TOKEN,
    "NOTION_DATABASE_ID": NOTION_DB_ID
}

missing_vars = [name for name, value in env_vars.items() if not value]
if missing_vars:
    print(f"âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing_vars)}")
    print("ğŸ”§ Render Dashboard â†’ Settings â†’ Environmentì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
    sys.exit(1)

print("âœ… ëª¨ë“  í™˜ê²½ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ")

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    notion_client = Client(auth=NOTION_TOKEN)
    print("âœ… API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    print(f"âŒ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    sys.exit(1)

# 1. ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘
def get_trending_keywords():
    try:
        print("ğŸ“¡ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì¤‘...")
        
        # ë°©ë²• 1: ì¤Œ(ZUM) ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´
        zum_url = "https://zum.com/"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        response = requests.get(zum_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì¶”ì¶œ ì‹œë„
        keywords = []
        
        # ì¤Œ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì„ íƒìë“¤ ì‹œë„
        selectors = [
            '.rank_keyword',
            '.keyword_list li',
            '.realtime_keyword',
            '[class*="keyword"]',
            '[class*="rank"]',
            'li',
            'span'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements:
                    text = elem.get_text().strip()
                    
                    # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° í•„í„°ë§
                    if text and len(text) > 1:
                        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
                        if text[0].isdigit():
                            # "1ì‹ ì§€ ë¬¸ì› ëŒì‹±" â†’ "ì‹ ì§€ ë¬¸ì› ëŒì‹±" 
                            cleaned_text = text[1:].strip()
                        else:
                            cleaned_text = text
                        
                        # ìœ íš¨í•œ í‚¤ì›Œë“œ ì¡°ê±´
                        if (cleaned_text and 
                            len(cleaned_text) > 2 and 
                            len(cleaned_text) < 50 and
                            cleaned_text not in keywords and
                            not cleaned_text.isdigit() and
                            'ë”ë³´ê¸°' not in cleaned_text and
                            'ì‹¤ì‹œê°„' not in cleaned_text and
                            'ZUM' not in cleaned_text.upper()):
                            
                            keywords.append(cleaned_text)
                            
                        if len(keywords) >= 5:
                            break
                
                if len(keywords) >= 5:
                    break
        
        # í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_keywords = []
        for keyword in keywords:
            # ê¸°ì¡´ í‚¤ì›Œë“œì™€ ë„ˆë¬´ ìœ ì‚¬í•œì§€ í™•ì¸
            is_similar = False
            for existing in unique_keywords:
                # 60% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                if len(set(keyword.split()) & set(existing.split())) / max(len(keyword.split()), len(existing.split())) > 0.6:
                    is_similar = True
                    break
            
            if not is_similar:
                unique_keywords.append(keyword)
            
            if len(unique_keywords) >= 5:
                break
        
        if unique_keywords:
            print(f"âœ… ì¤Œì—ì„œ ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ìˆ˜ì§‘: {unique_keywords}")
            return unique_keywords[:5]
        else:
            raise Exception("ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âš ï¸ ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # ë°±ì—…: í˜„ì¬ í™”ì œ í‚¤ì›Œë“œ (ìˆ˜ë™ ì—…ë°ì´íŠ¸)
        current_hot_keywords = [
            "ì‹ ì§€ ë¬¸ì›",
            "ìœ í€´ì¦ˆ ì´íš¨ë¦¬", 
            "ìƒë²• ê°œì •ì•ˆ",
            "ì´ˆë“±í•™ìƒ ë“œë¼ë§ˆ ë…¼ë€",
            "ì±„ìƒë³‘ íŠ¹ê²€"
        ]
        
        print(f"ğŸ“ˆ í˜„ì¬ í™”ì œ í‚¤ì›Œë“œ ì‚¬ìš©: {current_hot_keywords}")
        return current_hot_keywords



# 2. êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘
def collect_google_data(keyword):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        # êµ¬ê¸€ ê²€ìƒ‰
        search_url = f"https://www.google.com/search?q={keyword}&hl=ko"
        response = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê²€ìƒ‰ ê²°ê³¼ ì œëª© ìˆ˜ì§‘
        titles = []
        for h3 in soup.find_all('h3', limit=3):
            if h3.text.strip():
                titles.append(h3.text.strip())
        
        if not titles:
            titles = [f"{keyword} ê´€ë ¨ ìµœì‹  ë™í–¥", f"{keyword} ì‹œì¥ ë¶„ì„", f"{keyword} ì „ë§"]
        
        return {
            'search_results': titles,
            'keyword': keyword
        }
        
    except Exception as e:
        print(f"âš ï¸ êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({keyword}): {e}")
        return {
            'search_results': [f"{keyword} íŠ¸ë Œë“œ ë¶„ì„", f"{keyword} ì‹œì¥ ë™í–¥"],
            'keyword': keyword
        }

# 3. GPT ë¶„ì„
def analyze_with_gpt(keyword, google_data):
    try:
        search_text = " / ".join(google_data['search_results'])
        
        prompt = f"""
í‚¤ì›Œë“œ: {keyword}
ê´€ë ¨ ì •ë³´: {search_text}

ìœ„ í‚¤ì›Œë“œì™€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

ë‹ˆì¦ˆ: [ì´ í‚¤ì›Œë“œê°€ ë°˜ì˜í•˜ëŠ” ì†Œë¹„ìì˜ ì£¼ìš” ë‹ˆì¦ˆë‚˜ ê´€ì‹¬ì‚¬]
ìš”ì•½: [ë§ˆì¼€íŒ… ê´€ì ì—ì„œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 1-2ì¤„]
ì „ë§: [í–¥í›„ 3-6ê°œì›” íŠ¸ë Œë“œ ì „ë§]
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        result = response.choices[0].message.content.strip()
        print(f"âœ… '{keyword}' GPT ë¶„ì„ ì™„ë£Œ")
        return result
        
    except Exception as e:
        print(f"âš ï¸ '{keyword}' GPT ë¶„ì„ ì‹¤íŒ¨: {e}")
        return f"""ë‹ˆì¦ˆ: {keyword}ì— ëŒ€í•œ ì •ë³´ ìˆ˜ìš” ì¦ê°€
ìš”ì•½: {keyword} ê´€ë ¨ ê´€ì‹¬ë„ê°€ ë†’ì•„ì§€ê³  ìˆì–´ ë§ˆì¼€íŒ… ê¸°íšŒ ì¡´ì¬
ì „ë§: ì§€ì†ì ì¸ ê´€ì‹¬ ìœ ì§€ ì˜ˆìƒ"""

# 4. ë…¸ì…˜ ì—…ë¡œë“œ
def upload_to_notion(keyword, analysis_result):
    try:
        today = datetime.date.today().isoformat()
        
        # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
        lines = analysis_result.split('\n')
        parsed_data = {
            "í‚¤ì›Œë“œ": keyword,
            "ë‚ ì§œ": today,
            "ë‹ˆì¦ˆ": "",
            "ìš”ì•½": "",
            "ì „ë§": ""
        }
        
        for line in lines:
            line = line.strip()
            if line.startswith("ë‹ˆì¦ˆ:"):
                parsed_data["ë‹ˆì¦ˆ"] = line.replace("ë‹ˆì¦ˆ:", "").strip()
            elif line.startswith("ìš”ì•½:"):
                parsed_data["ìš”ì•½"] = line.replace("ìš”ì•½:", "").strip()
            elif line.startswith("ì „ë§:"):
                parsed_data["ì „ë§"] = line.replace("ì „ë§:", "").strip()
        
        # ë…¸ì…˜ í˜ì´ì§€ ìƒì„±
        notion_client.pages.create(
            parent={"database_id": NOTION_DB_ID},
            properties={
                "í‚¤ì›Œë“œ": {"title": [{"text": {"content": parsed_data["í‚¤ì›Œë“œ"]}}]},
                "ë‚ ì§œ": {"date": {"start": parsed_data["ë‚ ì§œ"]}},
                "ë‹ˆì¦ˆ": {"rich_text": [{"text": {"content": parsed_data["ë‹ˆì¦ˆ"]}}]},
                "ìš”ì•½": {"rich_text": [{"text": {"content": parsed_data["ìš”ì•½"]}}]},
                "ì „ë§": {"rich_text": [{"text": {"content": parsed_data["ì „ë§"]}}]},
            },
        )
        
        print(f"âœ… '{keyword}' ë…¸ì…˜ ì—…ë¡œë“œ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ '{keyword}' ë…¸ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    print("="*50)
    print("ğŸ¤– ìë™ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    try:
        # 1. í‚¤ì›Œë“œ ìˆ˜ì§‘
        keywords = get_trending_keywords()
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ: {keywords}")
        
        success_count = 0
        
        # 2. ê° í‚¤ì›Œë“œë³„ ë¶„ì„
        for i, keyword in enumerate(keywords, 1):
            print(f"\n[{i}/{len(keywords)}] ğŸ” '{keyword}' ë¶„ì„ ì¤‘...")
            
            # êµ¬ê¸€ ë°ì´í„° ìˆ˜ì§‘
            google_data = collect_google_data(keyword)
            
            # GPT ë¶„ì„
            analysis = analyze_with_gpt(keyword, google_data)
            
            # ë…¸ì…˜ ì—…ë¡œë“œ
            if upload_to_notion(keyword, analysis):
                success_count += 1
            
            print(f"âœ… '{keyword}' ì²˜ë¦¬ ì™„ë£Œ")
        
        print("\n" + "="*50)
        print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(keywords)}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
